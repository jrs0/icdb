---
title: "Development"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Development}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Library development 

This section contains notes about development, such as todo lists, etc.

### Development environment

The development environment is compatible with Windows, Linux and Mac (todo check Mac). To set up the R environment, install R and install the devtools package. You can use R studio or any other editor/IDE. The top level directory is an R package, compatible with devtools::load_all().

For Tauri/Next.js development, you need to install some dependencies. First, install the [Rust](https://www.rust-lang.org/tools/install) programming language for your operating system; this will give you the Rust compiler and the cargo package manager. On Windows, install Node.js from [here](https://nodejs.org/en/download/). On Linux and Mac, install the Node Version Manager (NVM) instead, by running:

```
# Pick the latest version
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
```

and use that to install Node as follows:

```
nvm install latest
```

When you have Node.js installed, you should have the `npm` command; check by running `npm -v`.

After you have cloned the repository, change into the map-editor directory and run

```
# Install node dependencies
npm install

# Install cargo tauri command
cargo install tauri-cli

# Build and run
cargo tauri dev

```

The installation of dependencies and first-time compilation may take a bit of time. When developing the map editor, you may need to add node dependencies. Do this by installing them using "npm install package-name" -- this will automatically update package.json, which will add the dependency to the project. Running "npm uninstall package-name" will remove the package from node_modules and delete the dependency from package.json. 

### Profiling

To profile C++ code, use gperftools on Linux. Download and build gperftools from source (obtaining libprofiler.so and the pprof binary as a result). You do not need to pass any options to the configure script. See the [gperftools page](https://gperftools.github.io/gperftools/cpuprofile.html), and the README and INSTALL files in the repository for more information. See also [this page](http://minimallysufficient.github.io/r/programming/c++/2018/02/16/profiling-rcpp-packages.html) for more a guide on how to set things up.

Once gperftools is installed (run pprof --version to check), run an R snippet like the following:

```R
start_profiler("/tmp/prof.out")
code_you_want_to_profile_involving_cpp()
stop_profiler()
```

Once the code has completed, run:

```bash
# For text output
pprof --text src/icdb.so /tmp/prof.out

# For graphical output
sudo apt install gv
pprof --gv src/icdb.so /tmp/prof.out 
```

An alternatie method of profiling is to use valgrind directly on an R script. Run:

```bash
R -d "valgrind --tool=callgrind" -f script.R
```

The -d flag runs R through the specified debugger. Since valgrind is a virtual machine, this will take much longer than just running the regular R script, so keep it as small as possible. The profile will also include things like devtools::load_all(), which is not of interest, but can also be easily ignored in kcachegrind later. Once the process is finished, open the output using 

```bash
kcachegrind callgrind.out.PID # Replace PID with the number
```

You will need to do some digging to find the function of interest. Look in the callee map for references to Rcpp, then work up or down from there.



### Todo list

This is the list of most important things:

- Fix the cache to flush to the level 2 cache at the end of the session (also refactor to R6)

Here is a list of other improvements that need to be made:

- Connect to the server, not a database. Arrange things so that the user can autocomplete databases before tables. Try to make it work so that the same database connection can be used in multiple dplyr pipe operations.
- Flush the level1 cache to the disk at some point (i.e. when the session ends) to shorten the load time next session. Make sure there is a clear way to disable this (it might not be a desirable default).
- Need to try to get autocomplete working in every context it makes sense (col names etc.).
- Look into replacing dbSendQuery with dbGetQuery for simplicity 
- Really need to find a way to lazily evaluate the contents of the Databases object. Currently, all the databases and tables are stored because the current autocomplete method rests of built-in autocompletion of lists -- however, this requires the list to be populated. It would be better to find a different autocomplete method that allowed lazy evaluation of completion options.
- Need to reconcile the mysql and sql server (microsoft) way of getting lists of databases and tables. mysql returns objects properly using dbListObjects, but sql server lists the databases in a table and then there is no clear way to get the tables without raw sql.
- Need to make a better error message when the user tries to get a non-existent table
- Need to test cache properly
- Need to add proper database disconnection code
- Should replace the server connection files with YAML not JSON (for consistency with everything else)
- Replace the testdata flag with proper logic for connecting to an SQLite database.
- Handle closing the database connection properly.
- Probably better to use the syntax table(srv$$dbname$tabname) instead of srv$dbname$tabname(). Then you could do srv$dbname$tabname %>% get_table(). Also, maybe there is a way to implement the non-parentheses methods even when there is a nested list -- if the \$ method could be made to check for a list, and do something different for a function.
- Found that the "SQL Server Native Client 11.0" works! Can update the documentation to reflect this.
- Want to add a way to specify a custom cache expiry, 

### Problem reports

- If a query fails (because it is invalid), the query looks like it might still getting written to the cache, so that next time you run the query, it thinks it has done it. This requires a bit of work to reproduce. Might need to look at the cache writing code order.

- Problem with parsing file paths that contain a directory component in the codes_from field of the mapping yaml file.

- Need to double check that the catch all codes are really working properly, and not classing everything in their subcategories as a catch all category. This will be solved by testing (not done yet). 

- install.packages complains about needing Rtools to build the package. Might be solved by making a binary package instead?

- Update the documentation to wrap pipe statements so you don't have to scroll horizontally.

## Documentation development

Add articles for the main package documentation in the vignettes folder, or use devtools::use_vignette("name") to create a template. 

## Notes

This section contains notes about things that work or anything else that might be helpful later.

### Renaming with janitor

This is a quick way to make good column names.

```{r, eval=FALSE}
tbl %>% rename_with(.fn = janitor::make_clean_names)
```
