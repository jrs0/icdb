---
title: "Verification strategies for predicted bleeding risks"
author: "John Scott"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r echo=FALSE}
library(tidyverse)
library(fabricatr)
library(GGally)
library(ggplot2)
library(dplyr)
library(purrr)

## Based on ARC-HBR validation paper (Cao et al.)

## Size of output / final dataset
n = 50000

## Proportion of population at high bleeding risk (HBR)
p_hbr <- 0.444

## Proportion of the HBR population at different bleeding risks
## The elements correspond to the probability of arc score
## 1, 2, 3, 4
p_arc_hbr = c(
    0.616,
    0.291,
    0.079,
    0.014
)

## Define the proportions of HBR patients to be in each HBR score category - central illustration figure
p_hbr_criteria = list(
    major_sev_anaemia = 0.332,
    major_oac = 0.185,
    major_malig = 0.168,
    major_sev_ckd = 0.137,
    major_surgery = 0.082,
    major_thrmcyt = 0.043,
    minor_age = 0.468,
    minor_mod_ckd = 0.396,
    minor_mild_anaemia = 0.369,
    minor_cva = 0.207,
    minor_bleed = 0.037
)

## Probability of bleed occurance for the baseline
## population (baseline bleeding risk), vs probability
## of bleed occurance for HBR
p_bleed_base <- 0.032

## Hazard ratios due to presence of multiple ARC HBR
## criteria. Each value in this vector represents the
## hazard ration of having that ARC HBR "score". The
## score is defined by adding up major criteria as
## 1 and minor criteria as 0.5. (See Central Illustration
## in paper)
arc_hr <- tribble(
    ~arc_score, ~hr,
     0,          1,
     1,          2.1,
     2,          3.88,
     3,          6.98,
     4,          12.26,
)

## Add the actual risk score based on the baseline
## bleeding risk
arc_risk <- arc_hr %>%
    mutate(risk = hr * p_bleed_base) %>%
    mutate(excess_risk = risk - p_bleed_base) %>%
    mutate(arc_score_squared = arc_score^2)

quadratic_risk_model <- lm(excess_risk ~ 0 + arc_score_squared, data = arc_risk)
arc_risk$excess_risk_prediction <- predict(quadratic_risk_model)

## Comment in to show the fit between the model and the excess risk
## ggplot(data=arc_risk, mapping=aes(x=arc_score)) +
##     geom_point(aes(y=excess_risk)) +
##     geom_smooth(aes(y=excess_risk_prediction),
##                 method = 'lm',
##                 formula = y ~ poly(x, 2))

## To use the model to predict risk, call this function
## arc_score here is 
estimate_risk <- function(arc_score) {
    newdata <- tibble(
        arc_score_squared = arc_score^2
    )
    ## Predict the excess risk, and add the baseline
    ## risk to get the risk estimate
    p_bleed_base + predict(quadratic_risk_model, newdata=newdata)
}

## Size of dataframe to sample from
n_sample = 10*n

set.seed(470)

## Create the data
hbr_data <- tibble(
    ## MAJOR criteria - from figure 1
    major_sev_anaemia = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_sev_anaemia"]]),
    major_oac = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_oac"]]),
    major_malig = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_malig"]]),
    major_sev_ckd = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_sev_ckd"]]),
    major_surgery = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_surgery"]]),
    major_thrmcyt = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["major_thrmcyt"]]),
    ## MINOR criteria - from figure 1
    minor_age = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["minor_age"]]),
    minor_mod_ckd = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["minor_mod_ckd"]]),
    minor_mild_anaemia = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["minor_mild_anaemia"]]),
    minor_cva = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["minor_cva"]]),
    minor_bleed = rbinom(
        n = n_sample, size = 1,
        prob = p_hbr_criteria[["minor_bleed"]])) %>%
    ## Calculate the number of times the HBR category
    ## has been satisfied (1 for a major and 0.5 for minor)
    mutate(maj_score = rowSums(select(., starts_with("major"))),
           min_score = rowSums(select(., starts_with("minor"))) %/% 2,
           arc_hbr_score = map2_dbl(maj_score, min_score, sum)) %>%
    ## Remove non-HBR rows 
    filter(arc_hbr_score != 0) %>%
    ## Remove high HBR score. Use the length of
    ## p_arc_hbr to get the upper limit
    filter(arc_hbr_score <= length(p_arc_hbr)) %>%
    ## apply the wanted proportions of each HBR score group
    group_by(arc_hbr_score) %>%
    ## This is slice_head with prop = p_arc_hbr[[arc_hbr_score]] * n
    ## However, slice_head does not support non-const prop
    slice(1:as.integer(p_arc_hbr[[first(arc_hbr_score)]] * p_hbr * n)) %>%
    ungroup()

## Compute the proportion of HBR vs. non-HBR rows
n_hbr <- nrow(hbr_data)
n_non_hbr <- n - n_hbr

## Insert zero rows to make up proportion of non-HBR
n_cols <- hbr_data %>% ncol()
mat <- matrix(integer(n_non_hbr * n_cols), nrow = n_non_hbr)
non_hbr_data <- as_tibble(mat)
names(non_hbr_data) <- hbr_data %>% colnames()
full_data <- rbind(hbr_data, non_hbr_data)

## Set risk noise
risk_noise = p_bleed_base / 10

## Add the risk computed using the quadratic risk model
full_data <- full_data %>%
    mutate(risk = estimate_risk(arc_hbr_score) + rnorm(n = nrow(full_data),
                                                       sd = risk_noise))

## Generate subsequent 12-month bleeding for each patient based
## on the bleeding risk (Bernoulli with p = bleeding risk)
full_data <- full_data %>%
    mutate(bleed = rbinom(n = n, size = 1,
                          prob = p_hbr_criteria[["major_oac"]]))

## Save for reference
saveRDS(full_data, "gendata/hbr_sim_dataset.R")

hbr_dataset <- full_data %>%
    filter(arc_hbr_score > 0)

## Check the size of the HBR group
p_hbr_in_data <- hbr_dataset %>%
    filter(arc_hbr_score > 0) %>%
    nrow() / nrow(full_data)
```

## Generating simulated high bleeding risk data

In total, the simulated dataset contains `r nrow(full_data)` rows, each representing a patient with particular major and minor risk factors for bleeding. Simulated data follows proportions in Cao et al. Major and minor risk factors are added up to create the ARC HBR score. The score is used to generate a probability of bleeding (bleeding risk) for each patient. One sample is then obtained from this binomial distribution, for each patient, representing whether a bleed occured or not. The following columns are present in the simulated data:

```{r echo=FALSE}
full_data %>% colnames()
```

In the simulation, the excess risk (additional risk above the baseline `r p_bleed_base`) depends quadratically on the ARC HBR score, following Cao et al. The following graph shows the simulated risks as a function of HBR score. The variation represents the risk noise added to each risk, a Gaussian distributed error centered on zero with a standard deviation of `r risk_noise`.

```{r}
ggplot(data=full_data, mapping=aes(x = arc_hbr_score, y = risk)) +
    geom_point()
```

The graph below shows the proportion of different HBR scores in the simulated data, vs. the target proportion taken from Cao et al.

```{r echo=FALSE}
## Plot the proportion of different HBR scores, compared
## to the target values in Cao et al.
hbr_score_prop <- hbr_dataset %>%
    mutate(n=n()) %>%
    group_by(arc_hbr_score) %>%
    summarise(prop = n()/first(n)) %>%
    left_join(data.frame(arc_hbr_score = seq_along(p_arc_hbr),
                         cao_prop = p_arc_hbr),
              by = "arc_hbr_score") %>%
    pivot_longer(cols = c("prop","cao_prop"),
                 names_to = "label", values_to = "value")

## Plot comparison
ggplot(data = hbr_score_prop) +
    geom_bar(mapping = aes(x = arc_hbr_score, y = value, fill = label), stat = "identity", position = "dodge") +
    ggtitle("Proportion of different ARC-HBR scores in the HBR group") + 
    theme_bw()
```

The graph below shows the breakdown of different major and minor ARC HBR criteria in the HBR group, matching the proportions in Cao et al.

```{r echo=FALSE}
## Check the prevalence of each major/minor criterion in the HBR group
hbr_criteria_prop <- hbr_dataset %>%
    mutate(n=n()) %>%
    summarise(across(matches("major|minor"), ~ sum(.x)/first(n))) %>%
    pivot_longer(cols = colnames(.), names_to = "label", values_to = "value") %>%
    mutate(class = "prop")
hbr_criteria_target <- tibble(label = paste0(names(p_hbr_criteria)),
                              value = unlist(p_hbr_criteria),
                              class = "target")
hbr_criteria_full <- rbind(hbr_criteria_prop, hbr_criteria_target)

## Reorder the factors
hbr_criteria_full <- hbr_criteria_full %>%
    mutate(label = factor(label, levels=c("minor_bleed",
                                          "minor_cva",
                                          "minor_mild_anaemia",
                                          "minor_mod_ckd",
                                          "minor_age",
                                          "major_thrmcyt",
                                          "major_surgery",
                                          "major_sev_ckd",
                                          "major_malig",
                                          "major_oac",
                                          "major_sev_anaemia")))

## Plot the comparison
ggplot(data = hbr_criteria_full) +
    geom_bar(mapping = aes(x = label, y = value, fill = class), stat = "identity", position = "dodge") +
    theme_bw() +
    coord_flip() +
    ggtitle("Prevalence of the ARC-HBR Criteria vs Cao et al. Within HBR Group")
```

In the simulated data, risk (probability) of bleeding depends quadratically on a linear combination of binary (0 or 1) input predictors $x$, the major and minor criteria: $$p_{bleed} = \text{p_bleed_base} + \alpha\left(\frac{1}{2}\sum x_{minor} + \sum x_{major}\right)^2$$. As a result, a logistic regression model is not appropriate for modelling the probability in this data. 

However, it is possible to use a logistic regression model anyway, to demonstrate that the variability in the logistic regression model is not a proxy for the confidence interval around the bleeding risk

## Using logistic regression to model bleeding risk

```{r echo=FALSE}
library(tidymodels)
library(probably)
library(discrim)
library(probably)

## For attempt to find optimal threshold
library(pROC)

##' Using the cross-validation resamples to obtain multiple models
##' and thereby obtain a spectrum of predicted class scoress. The
##' intent is to assess the variability in the predicted probabilities.
##'
##' @return A tibble with the test set, containing predictors retained
##' in the model and the predicted outcome, along with a model_id column
##' indicating which cross-validation model was used to make the predictions.
##' @author 
predict_resample <- function(train, test, folds, recipe)
{
    model <- logistic_reg() %>% 
        set_engine('glm') %>% 
        set_mode('classification')
    workflow <- workflow() %>%
        add_model(model) %>%
        add_recipe(recipe)
    ctrl_rs <- control_resamples(
        extract = function (x) extract_fit_parsnip(x)
    )
    fits <- workflow %>%
        fit_resamples(folds, control = ctrl_rs) %>%
        pull(.extracts) %>%
        map(~ .x %>% pluck(".extracts", 1))
    ## Use all the models developed on each cross-validation fold
    ## to predict probabilities in the training set
    pre <- recipe %>%
        prep() %>%
        bake(new_data = test) %>%
        ## The id is necessary later to pair up predictions from
        ## multiple different calls to this function
        mutate(id = as.factor(row_number()))
    ## For each bleeding model, predict the probabilities for
    ## the test set and record the model used to make the
    ## prediction in model_id
    pred_bleed <- list(
        n = seq_along(fits),
        f = fits
    ) %>%
        pmap(function(n, f)
        {
            f %>%
                augment(new_data = pre) %>%
                mutate(model_id = as.factor(n))
        }) %>%
        list_rbind()
}


## Obtain the dataset
hbr_sim_dataset <- readRDS("gendata/hbr_sim_dataset.R")

## dataset <- hbr_sim_dataset %>%
##     mutate(bleed_after = factor(bleed == 0, labels = c("bleed_occured", "no_bleed"))) %>%
##     select(-bleed) %>%
##     drop_na()
## summary(dataset)

set.seed(47)

## Train proportion
p_train <- 0.75

## Get a test training split stratified by bleeding (the
## less common outcome)
split <- initial_split(dataset, prop = p_train, strata = bleed_after)
train <- training(split)
test <- testing(split)

## Number of folds
n_folds <- 20

## Create cross-validation folds
folds <- vfold_cv(train,
                  v = n_folds,
                  strata = bleed_after)

## Create the recipe for the bleed models
recipe <- recipe(bleed_after ~ ., data = train) %>%
    update_role(arc_hbr_score, new_role = "arc_hbr_score") %>%
    update_role(maj_score, new_role = "maj_score") %>%
    update_role(min_score, new_role = "min_score") %>%
    update_role(risk, new_role = "risk") %>%
    step_nzv(all_predictors()) %>%
    step_center(all_predictors()) %>%
    step_scale(all_predictors())

## Perform the resampling predictions
pred <- predict_resample(train, test, folds, recipe)
```

In order to test the logistic regression model, the data were split into a training set (p = `r p_train`) and a test set, stratified according to bleeding outcome. Inside the training set `r n_folds` cross-validation folds were generated, and logistic regression models were fitted in each fold. The models resulting from each of these fits were used to make predictions for the bleeding risk in the training set. These risks can then be directly assessed according to two criteria:

* What variability exists among the risk predictions from the different models.
* Whether or not the risks are similar to the correct risks (the ones used to generate the data).

The graph below shows a plot of the risks predicted by the models vs. the true risks. The result shows that the true risk does not fall within the variability of the cross-validation models, meaning that variation due to resampling does not constitute a proxy for the confidence interval of the true bleeding risk.

```{r echo=FALSE}
## Plot the true risk vs the logistic regression probability. The importance
## of this plot is that it shows that the true risk does not necessarily fall
## within the variation of predicted class probabilities for models developed
## on cross-validation training folds. That variation is not related to the
## confidence interval for the true risk.
pred %>%
    ## Uncomment to view one model for all patients
    filter(model_id == 1) %>%
    slice_sample(prop = 0.25) %>%
    ## Uncomment to view all models for some patients
    ggplot(aes(x = risk,
               y = .pred_bleed_occured,
               color = model_id)) +
    geom_point() +
    geom_abline(slope=1, intercept=0) +
    xlim(0, 0.4) +
    ylim(0, 0.4)
```




